{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "mrdxfBovC93k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pdb\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaModel, BertTokenizer, BertModel\n",
    "import csv\n",
    "import re\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMHY6cKoDDoX",
    "outputId": "dd49a2c1-88ae-4fc6-e52b-dfa005ba62c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 35.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 62.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=b6e31bb0337bfb65c4770dc72f625ca174819bd70897c93b985b938fbbe32c74\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvmUfvdyDONi",
    "outputId": "beef1c6f-6268-4b1c-f4b9-d15d7c5891e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "hBGKUojYSuTy"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "#this is used to keep track of time during training\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "S-cmTNpmC93n",
    "outputId": "8b7db559-eff6-46f0-d13b-1b6cc529c758"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>33110</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6164</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>10899</td>\n",
       "      <td>State officials blast ' unprecedented ' DHS &lt;m...</td>\n",
       "      <td>idea</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>1781</td>\n",
       "      <td>Protesters Rally for &lt;Refugees/&gt; Detained at J...</td>\n",
       "      <td>stewardesses</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>5628</td>\n",
       "      <td>Cruise line Carnival Corp. joins the fight aga...</td>\n",
       "      <td>raisin</td>\n",
       "      <td>21000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>14483</td>\n",
       "      <td>Columbia police hunt woman seen with &lt;gun/&gt; ne...</td>\n",
       "      <td>cake</td>\n",
       "      <td>32200</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9651</th>\n",
       "      <td>5255</td>\n",
       "      <td>Here 's What 's In The House-Approved Health &lt;...</td>\n",
       "      <td>food</td>\n",
       "      <td>11000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  ... meanGrade\n",
       "0     14530  ...       0.2\n",
       "1     13034  ...       1.6\n",
       "2      8731  ...       1.0\n",
       "3        76  ...       0.4\n",
       "4      6164  ...       0.0\n",
       "...     ...  ...       ...\n",
       "9647  10899  ...       0.0\n",
       "9648   1781  ...       0.4\n",
       "9649   5628  ...       0.6\n",
       "9650  14483  ...       1.4\n",
       "9651   5255  ...       0.4\n",
       "\n",
       "[9652 rows x 5 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('drive/MyDrive/NLP_Coursework/ic_nlp_cw/task-1/train.csv')\n",
    "test_df = pd.read_csv('drive/MyDrive/NLP_Coursework/ic_nlp_cw/task-1/dev.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "0Qt6dU5rC93n"
   },
   "outputs": [],
   "source": [
    "#instead of having two inputs out of preprocessing, edit the dataset, and add in columns which we can use as inputs\n",
    "#we can also add an 'old' field which contains the original word \n",
    "train_df['old'] = train_df.apply(lambda x:x['original'][x['original'].find('<')+1:x['original'].find('>')-1],axis=1)\n",
    "test_df['old'] = test_df.apply(lambda x:x['original'][x['original'].find('<')+1:x['original'].find('>')-1],axis=1)\n",
    "\n",
    "#first we add a field to the data which contains the edited headline\n",
    "train_df['edited'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],\"??? \" + x['edit'] ) ,axis=1)\n",
    "test_df['edited'] = test_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],\"??? \" + x['edit'] ) ,axis=1)\n",
    "\n",
    "train_df['original'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],\"??? \" \n",
    "                                                                     +x['old']) ,axis=1)\n",
    "test_df['original'] = test_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1], \"??? \" +x['old']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "X4CV-pPyC93o",
    "outputId": "6dd002fa-bd04-4df7-c7b1-6c8bea98a9c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'All 22 ??? promises Trump made in his speech to Congress , in one chart [SEP] All 22 ??? sounds Trump made in his speech to Congress , in one chart'"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#and then we can add another field which includes the sentence with old word + SEP + new word\n",
    "#train_df['combined'] = train_df.apply(lambda x:x['edited'] + ' [SEP] From '+x['old'] + ' to '+x['edit'] ,axis=1)\n",
    "#test_df['combined'] = test_df.apply(lambda x:x['edited'] + ' [SEP] From '+x['old'] + ' to '+x['edit'] ,axis=1)\n",
    "train_df['combined'] = train_df.apply(lambda x:x['original'] + ' [SEP] ' + x['edited'] ,axis=1)\n",
    "test_df['combined'] = test_df.apply(lambda x:x['original'] + ' [SEP]+ ' +x['edited'] ,axis=1)\n",
    "train_df['combined'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "JyxwdcRpC93o"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "def tokenizer(data, column):\n",
    "    # Load the BERT tokenizer.\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "\n",
    "    # Get the lists of sentences and their labels  - using the finalized edited headline here\n",
    "    sentences = data[column].values\n",
    "    labels = data['meanGrade'].values\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            #add_special_tokens = True, # this adds a CLS at the begining of a sentence and SEP at the end\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks for the padded tokens\n",
    "                            return_tensors = 'pt',   # Return pytorch tensors.\n",
    "                            #truncation = True\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBYgY4BQC93p",
    "outputId": "e0104f16-9cf3-41db-a9a9-6b7666f540f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#we can decide which column of the dataset we want to train on - let's do the 'combined column'\n",
    "#we can compare with the single sentence column later\n",
    "train_ids, train_masks, train_labels = tokenizer(train_df, 'combined')\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "PKjJq-jYC93p"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#same as always\n",
    "train_and_dev = TensorDataset(train_ids, train_masks, train_labels)\n",
    "\n",
    "train_examples = round(len(train_and_dev)*train_proportion)\n",
    "dev_examples = len(train_and_dev) - train_examples\n",
    "train_dataset, dev_dataset = random_split(train_and_dev,\n",
    "                                           (train_examples,\n",
    "                                            dev_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "HGGc3XGYC93p"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order (shuffle)\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            dev_dataset, \n",
    "            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2P8PYfiC93p",
    "outputId": "5f20436c-92bf-46f8-9757-34485b31b45e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    " \n",
    "#for regression, we can actually use for BertForSequenceClassifier and set the number of labels to 1\n",
    "#which turns it into a regression problem\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 1,  \n",
    "    output_attentions = False, # don't return attention weights or hidden states\n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "#store double values\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "rXD1YH1eC93q"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-3, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "#BERT authors recommend between 2 and 4. \n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# The scheduler can actually learn the best learning rate throughout tranining\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "piYvxFK6Xi5U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUc9B3kEC93q",
    "outputId": "7cd3f2ab-4e22-4370-ee0a-929155851733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "tensor(0.3914, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5338, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3816, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3888, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4091, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5856, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2646, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3608, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3351, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3771, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4120, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2519, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3826, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3005, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2940, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3506, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3522, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3416, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3534, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3629, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5275, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3471, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5637, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3789, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5764, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3479, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3752, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3429, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3521, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4916, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2161, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3924, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3778, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3466, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2819, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2400, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3851, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2975, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3667, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4408, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    40  of    155.    Elapsed: 0:02:32.\n",
      "tensor(0.3468, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5567, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3318, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3214, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4457, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4508, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3623, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4382, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5852, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7177, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1344, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5342, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3698, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4864, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2048, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3977, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6412, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4768, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3964, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3119, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3670, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3192, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5800, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4540, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4422, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3593, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7418, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9041, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5419, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4048, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3649, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5073, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4229, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4784, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4131, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4513, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5613, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4044, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4333, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4502, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    80  of    155.    Elapsed: 0:05:04.\n",
      "tensor(0.4288, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1653, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5659, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5150, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5785, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3847, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5435, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4780, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5070, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2980, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3100, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4767, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7817, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5689, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3673, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3822, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2066, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2814, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4998, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3766, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3472, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3829, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5733, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3839, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2720, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4843, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2984, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2920, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3674, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2728, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3451, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4408, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2705, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3372, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3467, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3066, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5407, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3920, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2906, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch   120  of    155.    Elapsed: 0:07:36.\n",
      "tensor(0.2149, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5932, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5163, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4872, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3419, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3320, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5430, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4377, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4299, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3009, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4381, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3567, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4600, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3220, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2308, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3873, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3792, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3054, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5493, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4095, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2902, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2095, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3511, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4732, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5421, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3286, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2838, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5196, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4495, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4719, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3858, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3465, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5343, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6546, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9788, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epcoh took: 0:09:46\n",
      "\n",
      "Running Validation...\n",
      "tensor(0.5953, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4645, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6846, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4962, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5074, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5545, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4099, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4172, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6981, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5377, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.7149, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6092, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4509, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5026, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.8196, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5197, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6166, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3505, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6069, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6019, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6781, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2272, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3583, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6150, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6684, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5370, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6089, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5384, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4458, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6352, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3739, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5726, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5425, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6146, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6676, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5977, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6002, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.6257, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5569, device='cuda:0', dtype=torch.float64)\n",
      "  RMSE: 0.7446\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:00:48\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "tensor(0.8005, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3486, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3880, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4283, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4286, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3339, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5290, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3739, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4266, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4408, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3629, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5352, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5463, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3194, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2573, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6331, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7589, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4980, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6204, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4369, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2857, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5053, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6590, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5895, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7656, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4276, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4220, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3442, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6453, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6099, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4679, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3265, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3532, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2828, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4414, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2942, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2479, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5506, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2901, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3919, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    40  of    155.    Elapsed: 0:02:32.\n",
      "tensor(0.2729, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4273, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2700, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3909, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2520, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3077, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2945, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2985, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2354, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3793, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4818, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3945, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4524, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4473, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3125, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3325, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3844, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2020, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5068, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4909, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4694, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3717, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2957, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4470, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3019, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2965, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3284, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3591, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3265, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4118, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3229, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3413, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3445, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2903, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4204, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3772, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4824, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4284, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4056, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5831, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    80  of    155.    Elapsed: 0:05:04.\n",
      "tensor(0.5760, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3471, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3413, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3087, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3444, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4377, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6593, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3833, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2685, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3554, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2878, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3046, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3196, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3368, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4064, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3921, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4161, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4177, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3300, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4022, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3543, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3979, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3324, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4218, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3546, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3974, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5229, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3148, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3243, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3452, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3794, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3174, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3877, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2534, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4034, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2608, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4015, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3150, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4635, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2905, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch   120  of    155.    Elapsed: 0:07:36.\n",
      "tensor(0.4747, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2699, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2887, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4500, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3046, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3200, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4720, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4108, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2629, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4863, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3036, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3712, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3728, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3241, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4343, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3503, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4860, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4627, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2695, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3635, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6312, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3747, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2942, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4046, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3562, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2718, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3894, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4417, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3195, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5371, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4383, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3111, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3069, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3267, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4283, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epcoh took: 0:09:46\n",
      "\n",
      "Running Validation...\n",
      "tensor(0.3654, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3040, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4097, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3623, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3265, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3940, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2534, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2649, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3620, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3057, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5667, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3629, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2781, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2971, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3896, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.1958, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4111, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2472, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4464, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3781, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3542, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2975, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3326, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3279, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4221, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3438, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3422, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2697, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2805, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3072, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2890, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3223, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2881, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3296, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3417, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3310, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3947, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3958, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3352, device='cuda:0', dtype=torch.float64)\n",
      "  RMSE: 0.5824\n",
      "  Validation Loss: 0.34\n",
      "  Validation took: 0:00:48\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "tensor(0.3907, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4296, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4009, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3012, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3988, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3063, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3032, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2742, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3363, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3279, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3733, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3079, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3909, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4199, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3557, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3992, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4035, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3744, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4396, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2627, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3625, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3294, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2813, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3574, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3315, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4611, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4055, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3998, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3813, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4627, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3708, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3018, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3113, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4349, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3427, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3652, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3936, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3853, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3873, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3549, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    40  of    155.    Elapsed: 0:02:32.\n",
      "tensor(0.3643, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4313, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3019, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3297, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3834, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3267, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3744, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3863, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3297, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3229, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3581, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4418, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2731, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3968, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4273, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3948, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3485, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3040, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3838, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3044, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3779, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4685, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3058, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3519, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3611, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3040, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3824, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2919, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3572, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4228, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3647, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4553, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3947, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4160, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2165, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4240, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3275, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3520, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3903, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch    80  of    155.    Elapsed: 0:05:03.\n",
      "tensor(0.3468, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3171, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4482, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3049, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4487, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3686, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2914, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3430, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3584, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4055, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3477, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4274, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2670, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4817, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3985, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3922, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4723, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3880, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5383, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5173, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4433, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3581, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3478, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3053, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2945, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3647, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3837, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3956, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4311, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2895, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3431, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3379, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3457, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3847, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3649, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4068, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3252, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2806, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2249, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2143, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "  Batch   120  of    155.    Elapsed: 0:07:35.\n",
      "tensor(0.3802, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4792, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4398, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4663, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3076, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3675, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3306, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4198, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4373, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3696, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3332, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3994, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3662, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3193, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3107, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3113, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3668, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4725, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3876, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3322, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3259, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3959, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3373, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2859, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3661, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4145, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5041, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3693, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2647, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3649, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2983, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3385, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3300, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3366, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2746, device='cuda:0', dtype=torch.float64, grad_fn=<MseLossBackward>)\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epcoh took: 0:09:46\n",
      "\n",
      "Running Validation...\n",
      "tensor(0.3654, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3040, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4097, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3623, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3265, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3940, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2534, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2649, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3620, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3057, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.5667, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3629, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2781, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2971, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3896, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.1958, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4111, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2472, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4464, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3781, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3542, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2975, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3326, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3279, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.4221, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3438, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3422, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2697, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2805, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3072, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2890, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3223, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.2881, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3296, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3417, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3310, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3947, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3958, device='cuda:0', dtype=torch.float64)\n",
      "tensor(0.3352, device='cuda:0', dtype=torch.float64)\n",
      "  RMSE: 0.5824\n",
      "  Validation Loss: 0.34\n",
      "  Validation took: 0:00:48\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:31:43 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        output = model(b_input_ids, \n",
    "                                    token_type_ids=None, \n",
    "                                    attention_mask=b_input_mask, \n",
    "                                    labels=b_labels)\n",
    "        \n",
    "        loss, predictions = output[:2]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    y_pred = np.array([])\n",
    "    y_true = np.array([])\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "          #this is a sequence classifier object which contains loss and predictions \n",
    "          output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels) \n",
    "          loss, predictions = output[:2] \n",
    " \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        y_pred = np.append(y_pred,predictions)\n",
    "        y_true = np.append(y_true,label_ids)\n",
    "\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    print(\"  RMSE: {0:.4f}\".format(rmse))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. RMSE.': rmse,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "MliqwrwrWxTc",
    "outputId": "d544302f-f6e0-4875-b6e7-0602ae46d69e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxWdd3/8debYUd2xo1dBRGQdcLUTLNUspTKWwSxtLsyUUyzLG3TNO/bO7NFJUr7tRgg4o5lopVtJskM4gIIIqAOLiCLiKgsfn5/nENdjhfMAHNd55qZ9/PxOA/O+X7P8rkuj9dnzvle1+coIjAzM6upWdYBmJlZaXKCMDOzvJwgzMwsLycIMzPLywnCzMzycoIwM7O8nCDM7N8krZD0kazjsNLgBGENQvrB9aakjZJelvRrSXvl9P9aUkgaU2O7H6XtZ6XLLSVdK6k63dcKST/ewXG2TzcU7YWalRAnCGtIToqIvYBhwHDg0hr9S4DPbF+Q1BwYCzybs86lQAUwCmgPHAPMy3ecnGlSvb6KPNJYzUqKE4Q1OBHxMjCbJFHkuhf4gKTO6fJo4Ang5Zx13gfcFREvRmJFRNy8O3FIulzS7ZJulfS6pHmShub07y/pDkmrJS2X9KU8206VtAE4K8/+W0n6gaTnJb0i6WeS2qR9x6RXQd+Q9Gp65TMhZ9uOkm5Oj/2cpG9JapbT/wVJi9K4F0oakXPoYZKekPRa+tpa7877Yw2fE4Q1OJJ6AB8Fltboegu4BxiXLn8GqPnhPwe4SNK5kg6VpD0MZwxwG9AFmA7cLalF+mF8L/A40B34MHChpBNqbHs70AmYlmffVwP9SRLhQel+vpPTvy/QLW0/E7hR0sFp3/VAR+AA4GiS9+KzAJJOBS5P2zoAJwNrcvY7liS59gWGkCd5WRMREZ48lfwErAA2Aq8DAfwJ6JTT/2vge8AHgEdIPnRfAdoA/wDOStcrA84DHgbeBl4EzsxznPU50xd2ENPlwJyc5WbAS8BRwGHA8zXWvxT4Vc62f9vJ6xXwBnBgTtvhwPJ0/hhgK9Aup38m8O30NW4GBub0fRH4Szo/G7hgJ+/zGTnL3wd+lvV/f0/ZTL7vaQ3JJyLij5KOJvlrvRvJB/i/RcQ/JJUD3wR+FxFv5l4kRMQ2YDIwOb1d89/ALyU9GhGLco9Tx5heyNn3O5Kqgf1Jktj+knLjKwP+nm/bPMqBtkBVTvxK97Hduoh4I2f5ufTY3YAW6XJuX/d0vifvHpepKfeW3KZ0n9YE+RaTNTgR8VeSK4Yf7GCVqcBXeO/tpZr7eTMiJgPrgIG7GU7P7TPpbaUeJFclL5D8td8pZ2ofESfmhrCT/b4KvAkMytm+YySD9Nt1ltQuZ7lXeuxXgS1A7xp9K9P5F4ADd+1lWlPkBGEN1Y+B43IHhXNcBxwH/K1mh6QL0wHeNpKaSzqT5NtMj+1mHCMlfSr9FtKFJLet5gCPAq9L+np6rDJJgyW9ry47jYh3gJuAH0naO429e40xDIDvpl/dPQr4OHBbepU0E7hKUntJvYGLSBInwC+Ar0oaqcRB6Tpm7+IEYQ1SRKwmuUL4Tp6+tRHxp4jI9xf6JuBaktsor5KMR5wSEcty1rm3xu8g7tpJKPcAp5FchXwa+FREbEk/pD9OMsC8PD3WL0gGjuvq6yQD8XPSbzr9ETg4p//l9LgvkgxynxMRT6d955OMYSwjGYOZDvwSICJuA65K214H7iYZZDd7F+X/f8jMaiPpcuCgiDgjg2MfA0yNiB7FPrY1Hb6CMDOzvJwgzMwsL99iMjOzvHwFYWZmeTWaH8p169Yt+vTpk3UYZmYNSlVV1asRUZ6vr9EkiD59+lBZWZl1GGZmDYqk53bU51tMZmaWlxOEmZnl5QRhZmZ5OUGYmVleThBmZpaXE4SZmeXlBGFmZnk1+QTx9tZt/O99i6hetynrUMzMSkqTTxCrNrzN9H89z3nT5vH21m1Zh2NmVjKafILo2aUt15w6lMerX+N7v1tU+wZmZk1Ek08QAKMH78vZHzyA3855jnvmr6x9AzOzJqCgCULSaEmLJS2VdEme/h9Jmp9OSyStr9HfQVK1pBsKGSfA1044mFF9unDJHU+y5JXXC304M7OSV7AEIakMmAx8FBgIjJc0MHediPhyRAyLiGHA9cCdNXZzJXkePF8IzcuaccPpw2nXqjnnTK1i49tbi3FYM7OSVcgriFHA0ohYFhGbgRnAmJ2sPx64ZfuCpJHAPsADBYzxXfbu0Jrrxw9nxatv8PU7nsAPUzKzpqyQCaI78ELOcnXa9h6SegN9gT+ny82Aa4Gv7uwAks6WVCmpcvXq1fUS9OEHduXiEwbw+yde4tf/XFEv+zQza4hKZZB6HHB7RGz/num5wH0RUb2zjSLixoioiIiK8vK8z7vYLeccfQAfOWQfrvr9IqqeW1dv+zUza0gKmSBWAj1zlnukbfmMI+f2EnA4MEnSCuAHwGckXV2IIPORxLVjh7J/pzacN20er258u1iHNjMrGYVMEHOBfpL6SmpJkgRm1VxJ0gCgM/DI9raImBARvSKiD8ltppsj4j3fgiqkjm1aMOWMEazbtJkLZjzGtnc8HmFmTUvBEkREbAUmAbOBRcDMiFgg6QpJJ+esOg6YESU4Ijxo/45cOWYwDy9dw4//uCTrcMzMikol+Lm8WyoqKqJQz6T+2u2PM7Oyml+d9T4+NGDvghzDzCwLkqoioiJfX6kMUpe0K8YMZuB+Hbjw1vm8sNZF/cysaXCCqIPWLcqYcsYI3ongvOku6mdmTYMTRB317tqOa08dyhPVr3HFvQuzDsfMrOCcIHbB8YP25YtHH8C0fz3PXY/t9CcaZmYNnhPELrr4+IM5rG8XLr3zSRa/7KJ+ZtZ4OUHsouZlzbj+9OG0b92CiVOreP2tLVmHZGZWEE4Qu2Hv9q25Yfxwnlu7yUX9zKzRcoLYTYcd0JWvnXAw9z35Mr98eEXW4ZiZ1TsniD1w9gcP4PiB+/C/9y2icsXarMMxM6tXThB7QBLXnDqU7p3bcN50F/Uzs8bFCWIPdWzTgikTRrJ+0xa+dIuL+plZ4+EEUQ8G7t+B731iMP98dg0/fHBx1uGYmdULJ4h6cmpFT8a9ryeTH3qWPy16JetwzMz2mBNEPbr85EEM2r8DX3ZRPzNrBJwg6lHrFmVMmTASgInTqnhri4v6mVnDVdAEIWm0pMWSlkp6zxPhJP1I0vx0WiJpfdo+TNIjkhZIekLSaYWMsz716tqWH44dxlMrN/BdF/UzswaseaF2LKkMmAwcB1QDcyXNioh/f2pGxJdz1j8fGJ4ubgI+ExHPSNofqJI0OyLWFyre+vSRgfsw8ZgDmfKXZ6no3ZlTRvbIOiQzs11WyCuIUcDSiFgWEZuBGcCYnaw/HrgFICKWRMQz6fyLwCqgvICx1ruvHNefww/oyjfvfpKnX96QdThmZruskAmiO/BCznJ12vYeknoDfYE/5+kbBbQEns3Td7akSkmVq1evrpeg60vzsmZcN344HVq3YOLUeWxwUT8za2BKZZB6HHB7RLxrVFfSfsBvgc9GxDs1N4qIGyOiIiIqystL7wKjvH0rbjh9BM+v3cTXbnNRPzNrWAqZIFYCPXOWe6Rt+Ywjvb20naQOwO+Bb0bEnIJEWASj+nbhktEDuH/By/zi78uzDsfMrM4KmSDmAv0k9ZXUkiQJzKq5kqQBQGfgkZy2lsBdwM0RcXsBYyyKzx/Vl9GD9uXq+5/m0eUu6mdmDUPBEkREbAUmAbOBRcDMiFgg6QpJJ+esOg6YEe++/zIW+CBwVs7XYIcVKtZCk8T3Tx1Cry5tmTR9HqtefyvrkMzMaqXGcl+8oqIiKisrsw5jp55+eQOfmPwww3p2YurnDqN5WakMAZlZUyWpKiIq8vX5E6qIBuzbgas+cShzlq3l2geXZB2OmdlOOUEU2SkjezB+VC+m/OVZHlzoon5mVrqcIDJw2UkDGdy9AxfNnM/za1zUz8xKkxNEBrYX9WsmuaifmZUsJ4iM9OzSlh+dNpQFL27g8lkLsg7HzOw9nCAydOyAfTjvQwcyY+4L3Fb5Qu0bmJkVkRNExi467mCOOLAr37r7KRa+6KJ+ZlY6nCAyVtZMXDd+OJ3atmDitCpee9NF/cysNDhBlIBue7Vi8ukjWLnuTS6+7XEX9TOzkuAEUSIq+nThko8O4IGFr3Dj35ZlHY6ZmRNEKfncB/py4qH78v3Zi/nXsjVZh2NmTZwTRAmRxP+dMoTeXdoy6ZbHWLXBRf3MLDtOECWmfesWTDljJBvf2sqkWx5j67b3PCfJzKwonCBK0MH7tud/PjWYR5ev5ZoHFmcdjpk1UU4QJeqTw3sw4bBe/Pyvy3hgwctZh2NmTVBBE4Sk0ZIWS1oq6ZI8/T/KeSDQEknrc/rOlPRMOp1ZyDhL1XdOGsiQHh35ym2P89yaN7IOx8yamIIlCEllwGTgo8BAYLykgbnrRMSXI2JYRAwDrgfuTLftAlwGHAaMAi6T1LlQsZaqVs3LmHz6CJpJnDN1nov6mVlRFfIKYhSwNCKWRcRmYAYwZifrjwduSedPAB6MiLURsQ54EBhdwFhLVs8ubfnxacNY9NIGvnPPU1mHY2ZNSCETRHcgtwJdddr2HpJ6A32BP+/qtk3BhwbszfnHHsTMympmznVRPzMrjlIZpB4H3B4Ru3QPRdLZkiolVa5evbpAoZWGCz/Snw8c1I1v3/MUT618LetwzKwJKGSCWAn0zFnukbblM47/3F6q87YRcWNEVERERXl5+R6GW9rKmomfjBtG57YtOXfaPBf1M7OCK2SCmAv0k9RXUkuSJDCr5kqSBgCdgUdymmcDx0vqnA5OH5+2NWld92rF5AkjeHH9m3xl5uO8846L+plZ4RQsQUTEVmASyQf7ImBmRCyQdIWkk3NWHQfMiJwSphGxFriSJMnMBa5I25q8kb07840TD+GPi17h5y7qZ2YFpMZSWrqioiIqKyuzDqMoIoJJtzzGH558iWmffz+HH9g165DMrIGSVBURFfn6SmWQ2nbB9qJ+fbu143wX9TOzAnGCaKD2atWcKWeM5I23tzJp+mNscVE/M6tnThANWP992nP1KYfy6Iq1XDPbRf3MrH45QTRwY4Z159Pv782Nf1vG/U+5qJ+Z1R8niEbgWx8/hKE9O3HxbY+z/FUX9TOz+uEE0QgkRf2GU1YmJk6t4s3NLupnZnvOCaKR6NE5Keq3+JXX+fY9T9FYvr5sZtlxgmhEjjl4b84/th+3V1Vzq4v6mdkecoJoZC74cD+O6teN78xa4KJ+ZrZHnCAamaSo33C6tmvJOVOreG2Ti/qZ2e5xgmiEurRryeQJI3hlw1tcNHO+i/qZ2W5xgmikRvTqzDdPPIQ/Pb2KKX99NutwzKwBcoJoxM48og8nDd2fax9YzD+ffTXrcMysgXGCaMQkcfWnDuWA8r340i2P8fJrLupnZnXnBNHItWvVnJ+dMYJNm7cxafo8F/UzszpzgmgCDtq7PVefMoTK59bxf394OutwzKyBKGiCkDRa0mJJSyVdsoN1xkpaKGmBpOk57d9P2xZJuk6SChlrY3fy0P058/De/OIfy/nDky9lHY6ZNQAFSxCSyoDJwEeBgcB4SQNrrNMPuBQ4MiIGARem7UcARwJDgMHA+4CjCxVrU/HNjw1kWM9OXHz7EyxbvTHrcMysxBXyCmIUsDQilkXEZmAGMKbGOl8AJkfEOoCIWJW2B9AaaAm0AloArxQw1iahZfNmTJ4wghZl4txp81zUz8x2qpAJojuQWxCoOm3L1R/oL+lhSXMkjQaIiEeAh4CX0ml2RCyqeQBJZ0uqlFS5evXqgryIxqZ7pzb8ZNxwFr/yOt+860kX9TOzHcp6kLo50A84BhgP3CSpk6SDgEOAHiRJ5VhJR9XcOCJujIiKiKgoLy8vYtgN2wf7l3PBh/tx52Mrmf7o81mHY2YlqpAJYiXQM2e5R9qWqxqYFRFbImI5sIQkYXwSmBMRGyNiI/AH4PACxtrkfOnYfnywfznfnbWQJ6rXZx2OmZWgQiaIuUA/SX0ltQTGAbNqrHM3ydUDkrqR3HJaBjwPHC2puaQWJAPU77nFZLuvWTPx49OG0W2vlkycOo/1mzZnHZKZlZiCJYiI2ApMAmaTfLjPjIgFkq6QdHK62mxgjaSFJGMOF0fEGuB24FngSeBx4PGIuLdQsTZVXdq15KdnjGTV62/x5Vtd1M/M3k2NZZCyoqIiKisrsw6jQfrtIyv49j0L+Orx/Zl0bL+swzGzIpJUFREV+fqyHqS2EnDG+3szZtj+/PDBJTy81EX9zCzhBGFI4n8/dSgHuqifmeVwgjAA2rZszpQzRvLWlm2c56J+ZkYtCULSsTnzfWv0fapQQVk2Dtp7L/7vv4ZQ9dw6/vc+F/Uza+pqu4L4Qc78HTX6vlXPsVgJ+PiQ/TnriD788uHl/P4JF/Uza8pqSxDawXy+ZWskvnHiIYzo1Ymv3f44z7qon1mTVVuCiB3M51u2RmJ7Ub9WLcqYOLWKTZu3Zh2SmWWgtgRxgKRZku7Nmd++3LeWba0B269jG34ybhjPrNrIN+50UT+zpqh5Lf255bl/UKOv5rI1Mkf1K+fLH+nPDx9cwsg+Xfj0+3tnHZKZFdFOE0RE/DV3Oa2LNBhYmfPsBmvEJn3oIOY9v44r713IkO4dGdqzU9YhmVmR1PY1159JGpTOdySpi3Qz8Jik8UWIzzLWrJn40dhhlLdvxbnT5rHuDRf1M2sqahuDOCoiFqTznwWWRMShwEjgawWNzEpG53Yt+emEEax+/W2+PNNF/cyaitoSRO6fi8eRlOcmIl4uWERWkob27MR3ThrIXxav5oaHlmYdjpkVQW0JYr2kj0saDhwJ3A8gqTnQptDBWWmZcFgvPjm8Oz/64xL+/owf8WrW2NWWIL5I8kyHXwEX5lw5fBj4fSEDs9Ijias+OZh+e+/FBTPm8+L6N7MOycwKaKcJIiKWRMToiBgWEb/OaZ8dEV+pbeeSRktaLGmppEt2sM5YSQslLZA0Pae9l6QHJC1K+/vU+VVZwWwv6rd56zucN30em7e6qJ9ZY7XTr7lKum5n/RHxpZ1sWwZMJhm7qAbmSpoVEQtz1ukHXAocGRHrJO2ds4ubgasi4kFJewH+JCoRB5bvxff/awjnTpvH/9y3iMtPHpR1SGZWALX9UO4c4ClgJvAiu1Z/aRSwNCKWAUiaQfLDu4U563wBmBwR6wC2/7ZC0kCgeUQ8mLa7IFCJOfHQ/fjvI/vyy4eXM7J3Z04aun/WIZlZPattDGI/4EbgBODTQAvgnoj4TUT8ppZtuwMv5CxXp225+gP9JT0saY6k0Tnt6yXdKekxSdekVyTvIulsSZWSKlev9qBpsV164gBG9u7M1+94gqWrXs86HDOrZ7WNQayJiJ9FxIdIfgfRCVgo6dP1dPzmQD/gGGA8cJOkTmn7UcBXgfcBBwBn5YnvxoioiIiK8vLyegrJ6qpFWTMmnz6CNi3KOGfqPN5420X9zBqTOj1RTtII4ALgDOAPQFUdNlsJ9MxZ7pG25aoGZkXElohYDiwhSRjVwPyIWBYRW0l+fzGiLrFace3bsTXXjR/OstUbudRF/cwaldpKbVwhqQq4CPgrUBERn8sdaN6JuUA/SX0ltQTGAbNqrHM3ydUDkrqR3Fpalm7bSdL2y4JjeffYhZWQIw/qxkXH9WfW4y/y2znPZR2OmdWT2gapvwUsB4am0/9IgmSwOiJiyI42jIitkiYBs4Ey4JcRsUDSFUBlRMxK+46XtBDYBlwcEWsAJH0V+JOSA1YBN+3B67QCO/eYg5j3/Hqu/N1CDu3ekeG9OmcdkpntIe3sloCkndZ3joiS+XOxoqIiKisrsw6jSVu/aTMfv/4fvPNO8LsvHUWXdi2zDsnMaiGpKiIq8vXVNkj9XL6J5NtJHyhEsNZwdWrbkikTRvLqxs1ceOt8trmon1mDVtsYRAdJl0q6QdLxSpxPMk4wtjghWkNyaI+OXH7yIP62ZDXX//mZrMMxsz1Q2xjEb4F1wCPA54FvkIw/fCIi5hc4Nmugxo/qSeVza/nJn55heK/OHN3fX0E2a4hqfSZ1RJwVET8n+Z3CQOAEJwfbGUlc9YlDOXif9lw44zFWuqifWYNUW4LYsn0mIrYB1RHxVmFDssagTcsyfjphBFu2BedNc1E/s4aotgQxVNKGdHodGLJ9XtKGYgRoDdcB5Xvxg1OHMP+F9Vz1e/+Mxayhqe1bTGUR0SGd2kdE85z5DsUK0hqu0YP34/Mf6MtvHnmOe+bX/CG9mZWyOpXaMNsTX//oAN7XpzOX3PEkz7zion5mDYUThBVci7Jm3HD6CNq1KuOcqVVsdFE/swbBCcKKYp8OSVG/5a++wSV3POGifmYNgBOEFc0RB3bjK8cfzO+eeInf/HNF1uGYWS2cIKyoJh59IB8esDdX3beIec+vyzocM9sJJwgrqmbNxA/HDmPfjq05b9o81mx8O+uQzGwHnCCs6Dq2bcGUCSNZ84aL+pmVMicIy8Tg7h254uRB/P2ZV/nJn1zUz6wUOUFYZk57X0/+a2QPrv/zM/xl8aqswzGzGgqaICSNlrRY0lJJl+xgnbGSFkpaIGl6jb4Okqol3VDIOC0bkrhyzOCkqN+t86letynrkMwsR8EShKQyYDLwUZIqsOMlDayxTj/gUuDIiBgEXFhjN1cCfytUjJa9Ni3L+NkZI9mWFvV7e+u2rEMys1QhryBGAUsjYllEbAZmAGNqrPMFYHJErAOIiH/fZ5A0EtgHeKCAMVoJ6NOtHdecOpTHq1/jyt+5qJ9ZqShkguhO8mjS7arTtlz9gf6SHpY0R9JoAEnNgGuBr+7sAJLOllQpqXL16tX1GLoV2+jB+3L2Bw9g6pznufsxF/UzKwVZD1I3B/oBx5A8kOgmSZ2Ac4H7IqJ6ZxtHxI0RURERFeXlfmpZQ/e1Ew5mVJ8uXHrnkyxxUT+zzBUyQawEeuYs90jbclUDsyJiS0QsB5aQJIzDgUmSVgA/AD4j6eoCxmoloHlZM244fTjtWjV3UT+zElDIBDEX6Cepr6SWwDhgVo117ia5ekBSN5JbTssiYkJE9IqIPiS3mW6OiLzfgrLGZe8Orbl+/HBWvPoGX7/dRf3MslSwBBERW4FJwGxgETAzIhZIukLSyelqs4E1khYCDwEXR8SaQsVkDcPhB3bl4hMG8PsnX+JXD6/IOhyzJkuN5S+0ioqKqKyszDoMqycRwRduruIvi1dx6xffz8jeXbIOyaxRklQVERX5+rIepDbLSxLXjh3K/p3acN60x3jVRf3Mis4JwkpWxzYtmHLGCNZt2swFMx5zUT+zInOCsJI2aP+OXDlmMA8vXcOP/7gk63DMmhQnCCt5Y9/Xk7EVPbj+z0t56GkX9TMrFicIaxCuGDOYgft14MJb5/PCWhf1MysGJwhrEFq3KGPKGSN4J4Jzp83jrS0u6mdWaE4Q1mD07tqOa08dypMrX+MKF/UzKzgnCGtQjh+0L188+gCm/+t57py301JdZraHnCCswbn4+IM5rG8XvnHXkzz98oaswzFrtJwgrMFpXtaM608fTvvWLZg4dR6vv7Ul65DMGiUnCGuQ9m7fmhvGD+f5tZv4mov6mRWEE4Q1WIcd0JWvnXAwf3jqZf7fP5ZnHY5Zo+MEYQ3a2R88gOMH7sPVf3iayhVrsw7HrFFxgrAGTRI/GDuUHp3bcN70eS7qZ1aPnCCswevQugU/nTCS9Zu28KVbXNTPrL4UNEFIGi1psaSlkvI+EU7SWEkLJS2QND1tGybpkbTtCUmnFTJOa/gG7t+B731iMP98dg0/fHBx1uGYNQrNC7VjSWXAZOA4kmdPz5U0KyIW5qzTD7gUODIi1knaO+3aBHwmIp6RtD9QJWl2RKwvVLzW8J1a0ZOq59Yx+aFnGdGrMx8+ZJ+sQzJr0Ap5BTEKWBoRyyJiMzADGFNjnS8AkyNiHUBErEr/XRIRz6TzLwKrgPICxmqNxOUnD2LQ/h348q3zeX6Ni/qZ7YlCJojuwAs5y9VpW67+QH9JD0uaI2l0zZ1IGgW0BJ7N03e2pEpJlatXr67H0K2hat2ijCkTRgJw7vQqF/Uz2wNZD1I3B/oBxwDjgZskddreKWk/4LfAZyPinZobR8SNEVERERXl5b7AsESvrm354dhhPLVyA9+9d0HW4Zg1WIVMECuBnjnLPdK2XNXArIjYEhHLgSUkCQNJHYDfA9+MiDkFjNMaoY8M3IeJxxzILY++wO1VLupntjsKmSDmAv0k9ZXUEhgHzKqxzt0kVw9I6kZyy2lZuv5dwM0RcXsBY7RG7CvH9efwA7ryzbueZNFLLupntqsKliAiYiswCZgNLAJmRsQCSVdIOjldbTawRtJC4CHg4ohYA4wFPgicJWl+Og0rVKzWODUva8Z144fTsU0LJk6tYoOL+pntEjWWImcVFRVRWVmZdRhWgh5dvpbxN83huEP2YcoZI5CUdUhmJUNSVURU5OvLepDarOBG9e3CJaMHcP+Cl/nF313Uz6yunCCsSfj8UX0ZPWhfrr7/aR5d7qJ+ZnXhBGFNgiSuOXUIvbq0ZdL0eax6/a2sQzIreU4Q1mS0b92CKWeMYMNbSVG/rdve89MaM8vhBGFNyoB9O3DVJw5lzrK1XPvgkqzDMStpThDW5JwysgfjR/Viyl+e5cGFr2QdjlnJcoKwJumykwYyuHsHLpo5n+fWvJF1OGYlyQnCmqTtRf2aSUycOs9F/czycIKwJqtnl7b86LShLHxpA5fd46J+ZjU5QViTduyAfTjvQwdya+ULzKx8ofYNzJoQJwhr8i467mCOOLAr3777KRa8+FrW4ZiVDCcIa/LKmonrxg+nU9sWnDttHq+96aJ+ZuAEYQZAt71aMfn0Eaxc9yYX3/Y4jaWIpdmecIIwS1X06cKlJx7CAwtf4ca/Lcs6HLPMOUGY5fjvI/vwsUP34/uzF/OvZWuyDscsUwVNEJJGS1osaamkS3awzlhJCyUtkDQ9p/1MSc+k05mFjNNsO0lcfcqh9O7Slkm3PAR/wKIAAAu+SURBVMaqDS7qZ01XwRKEpDJgMvBRYCAwXtLAGuv0Ay4FjoyIQcCFaXsX4DLgMGAUcJmkzoWK1SxXUtRvJBvf2sokF/WzJqyQVxCjgKURsSwiNgMzgDE11vkCMDki1gFExKq0/QTgwYhYm/Y9CIwuYKxm73Lwvu35n08N5tHla7lm9uKswzHLRCETRHcg95dH1Wlbrv5Af0kPS5ojafQubIuksyVVSqpcvXp1PYZuBp8c3oMJh/Xi539bxuwFL2cdjlnRZT1I3RzoBxwDjAduktSprhtHxI0RURERFeXl5QUK0Zqy75w0kCE9OvLVmY+z4lUX9bOmpZAJYiXQM2e5R9qWqxqYFRFbImI5sIQkYdRlW7OCa9W8jMmnj6BZMzFxmov6WdNSyAQxF+gnqa+klsA4YFaNde4muXpAUjeSW07LgNnA8ZI6p4PTx6dtZkXXs0tbfnzaMBa9tIFv3/1U1uGYFU3BEkREbAUmkXywLwJmRsQCSVdIOjldbTawRtJC4CHg4ohYExFrgStJksxc4Iq0zSwTHxqwN+cfexC3VVVz69znsw7HrCjUWEoKVFRURGVlZdZhWCO27Z3gzF8+yqMr1nLnxCMY3L1j1iGZ7TFJVRFRka8v60FqswajrJn4ybhhdGnb0kX9rElwgjDbBV33asXkCSN4cf2bfGXm47zzTuO4AjfLxwnCbBeN7N2Zb37sEP646BV+7qJ+1og5QZjthrOO6MPHhuzHNbOf5pFnXdTPGicnCLPdIIn/O2UIfbu14/xbHuMVF/WzRsgJwmw37dWqOVPOGMkbb29l0vR5bHFRP2tknCDM9kD/fdpz9SmHMnfFOr5//9NZh2NWr5wgzPbQmGHd+fT7e3PT35dz/1MvZR2OWb1xgjCrB9/6+CEM7dmJi297guUu6meNhBOEWT1IivoNp6xMTJxaxZubXdTPGj4nCLN60qNzUtRv8Suv8627n6KxlLGxpssJwqweHXPw3px/bD/umFfNjLkv1L6BWQlzgjCrZxd8uB9H9evGZbMW8NTK17IOx2y3OUGY1bOkqN9wurZryTlTq3htk4v6WcPkBGFWAF3atWTyhBG8suEtLpo530X9rEFqnnUAZo3ViF6d+dbHBnLZrAV86Nq/0LLMf49ZYQzYrwPXjx9e7/staIKQNBr4CVAG/CIirq7RfxZwDf953vQNEfGLtO/7wMdIrnIeBC4Ify3EGpjPHN6bTZu38eTK9VmHYo1Yz85tCrLfgiUISWXAZOA4oBqYK2lWRCysseqtETGpxrZHAEcCQ9KmfwBHA38pVLxmhSCJicccmHUYZrulkNe8o4ClEbEsIjYDM4Axddw2gNZAS6AV0AJ4pSBRmplZXoVMEN2B3C+CV6dtNZ0i6QlJt0vqCRARjwAPAS+l0+yIWFRzQ0lnS6qUVLl69er6fwVmZk1Y1qNm9wJ9ImIIyTjDbwAkHQQcAvQgSSrHSjqq5sYRcWNEVERERXl5eRHDNjNr/AqZIFYCPXOWe/CfwWgAImJNRLydLv4CGJnOfxKYExEbI2Ij8Afg8ALGamZmNRQyQcwF+knqK6klMA6YlbuCpP1yFk8Gtt9Geh44WlJzSS1IBqjfc4vJzMwKp2DfYoqIrZImAbNJvub6y4hYIOkKoDIiZgFfknQysBVYC5yVbn47cCzwJMmA9f0RcW+hYjUzs/dSY/lpQUVFRVRWVmYdhplZgyKpKiIq8vVlPUhtZmYlqtFcQUhaDTy3B7voBrxaT+HUJ8e1axzXrnFcu6YxxtU7IvJ+DbTRJIg9JalyR5dZWXJcu8Zx7RrHtWuaWly+xWRmZnk5QZiZWV5OEP9xY9YB7IDj2jWOa9c4rl3TpOLyGISZmeXlKwgzM8vLCcLMzPJq9AlC0i8lrZL01A76Jek6SUvTsuMjcvrOlPRMOp1Z5LgmpPE8Kemfkobm9K1I2+dLqtefj9chrmMkvZYee76k7+T0jZa0OH0vLylyXBfnxPSUpG2SuqR9hXy/ekp6SNJCSQskXZBnnaKeY3WMKavzqy6xFf0cq2NcRT/HJLWW9Kikx9O4vptnnVaSbk3fk39J6pPTd2navljSCbscQEQ06gn4IDACeGoH/SeSVIsV8H7gX2l7F2BZ+m/ndL5zEeM6YvvxgI9ujytdXgF0y+j9Ogb4XZ72MuBZ4ACSBz09DgwsVlw11j0J+HOR3q/9gBHpfHtgSc3XXexzrI4xZXV+1SW2op9jdYkri3MsPWf2SudbAP8C3l9jnXOBn6Xz40ie0gkwMH2PWgF90/eubFeO3+ivICLibySFAHdkDHBzJOYAnZRUmT0BeDAi1kbEOpLnVYwuVlwR8c/0uABzSMqlF1wd3q8d2ZMnCNZ3XOOBW+rr2DsTES9FxLx0/nWSqsM1H4xV1HOsLjFleH7V5f3akYKdY7sRV1HOsfSc2Zgutkinmt8sGkP6LB2SQqcflqS0fUZEvB0Ry4GlJO9hnTX6BFEHO3ryXV2fiFcMnyP5C3S7AB6QVCXp7AziOTy95P2DpEFpW0m8X5LaknzI3pHTXJT3K720H07yV16uzM6xncSUK5Pzq5bYMjvHanvPin2OSSqTNB9YRfIHxQ7Pr4jYCrwGdKUe3q+Clfu2+iHpQyT/A38gp/kDEbFS0t7Ag5KeTv/CLoZ5JLVbNko6Ebgb6FekY9fFScDDEZF7tVHw90vSXiQfGBdGxIb63PfuqktMWZ1ftcSW2TlWx/+ORT3HImIbMExSJ+AuSYMjIu9YXH3zFcSOn3xX6xPxCk3SEJIn7Y2JiDXb2yNiZfrvKuAudvGycU9ExIbtl7wRcR/QQlI3SuD9So2jxqV/od8vJQ+1ugOYFhF35lml6OdYHWLK7PyqLbaszrG6vGepop9j6b7XAw/x3tuQ/35fJDUHOgJrqI/3q74HVUpxAvqw40HXj/HuAcRH0/YuwHKSwcPO6XyXIsbVi+Se4RE12tsB7XPm/wmMLmJc+/KfH1iOInn6n0iuRpeRDIZtH0AcVKy40v6OJOMU7Yr1fqWv/WbgxztZp6jnWB1jyuT8qmNsRT/H6hJXFucYUA50SufbAH8HPl5jnfN49yD1zHR+EO8epF7GLg5SN/pbTJJuIflWRDdJ1cBlJAM9RMTPgPtIvmWyFNgEfDbtWyvpSpJHpwJcEe++pCx0XN8huY/402S8ia2RVGvch+QyE5L/YaZHxP1FjOu/gImStgJvAuMiORvzPkGwiHFB8izzByLijZxNC/p+AUcCnwaeTO8TA3yD5AM4q3OsLjFlcn7VMbYszrG6xAXFP8f2A34jqYzkjs/MiPid3v1kzv8H/FbSUpLkNS6NeYGkmcBCkqd2nhfJ7ao6c6kNMzPLy2MQZmaWlxOEmZnl5QRhZmZ5OUGYmVleThBmZpaXE4RZLdKqnfNzpvqsItpHO6hQa5a1Rv87CLN68GZEDMs6CLNi8xWE2W5KnwHw/fQ5AI9KOiht7yPpz0qet/AnSb3S9n0k3ZUWoXtc0hHprsok3ZTW+39AUpt0/S8peT7BE5JmZPQyrQlzgjCrXZsat5hOy+l7LSIOBW4Afpy2XQ/8JiKGANOA69L264C/RsRQkmdbbP8VcD9gckQMAtYDp6TtlwDD0/2cU6gXZ7Yj/iW1WS0kbYyIvfK0rwCOjYhlaaG3lyOiq6RXgf0iYkva/lJEdJO0GugREW/n7KMPSQnnfuny14EWEfE9SfcDG0mqmd4d/3kugFlR+ArCbM/EDuZ3xds589v4z9jgx4DJJFcbc9NKnWZF4wRhtmdOy/n3kXT+n6QF04AJJBU4Af4ETIR/PwSm4452KqkZ0DMiHgK+TlJF9D1XMWaF5L9IzGrXJqfCJ8D9EbH9q66dJT1BchUwPm07H/iVpIuB1aTVW4ELgBslfY7kSmEi8NIOjlkGTE2TiIDrInkegFnReAzCbDelYxAVEfFq1rGYFYJvMZmZWV6+gjAzs7x8BWFmZnk5QZiZWV5OEGZmlpcThJmZ5eUEYWZmef1/3YREzO4YYQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "rmse = []\n",
    "\n",
    "for e in training_stats:\n",
    "  epochs.append(e['epoch'])\n",
    "  training_loss.append(e['Training Loss'])\n",
    "  validation_loss.append(e['Valid. Loss'])\n",
    "  rmse.append(e['Valid. RMSE.'])\n",
    "\n",
    "plt.plot(epochs, training_loss, color = 'blue', label = 'training loss')\n",
    "plt.plot(epochs, validation_loss, color = 'green', label = 'validation loss')\n",
    "plt.title(\"Training and validation loss per epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, rmse)\n",
    "plt.title(\"RMSE per epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNoZM_9LC93r",
    "outputId": "db2553ef-e6dc-49f0-f65b-2f1626126fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    242.    Elapsed: 0:01:36.\n",
      "  Batch    80  of    242.    Elapsed: 0:03:13.\n",
      "  Batch   120  of    242.    Elapsed: 0:04:49.\n",
      "  Batch   160  of    242.    Elapsed: 0:06:25.\n",
      "  Batch   200  of    242.    Elapsed: 0:08:02.\n",
      "  Batch   240  of    242.    Elapsed: 0:09:38.\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Training epcoh took: 0:09:41\n",
      "\n",
      "Running Validation...\n",
      "  RMSE: 1.1461\n",
      "  Validation Loss: 1.32\n",
      "  Validation took: 0:00:49\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    242.    Elapsed: 0:01:36.\n",
      "  Batch    80  of    242.    Elapsed: 0:03:12.\n",
      "  Batch   120  of    242.    Elapsed: 0:04:49.\n",
      "  Batch   160  of    242.    Elapsed: 0:06:25.\n",
      "  Batch   200  of    242.    Elapsed: 0:08:02.\n",
      "  Batch   240  of    242.    Elapsed: 0:09:38.\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Training epcoh took: 0:09:41\n",
      "\n",
      "Running Validation...\n",
      "  RMSE: 1.1461\n",
      "  Validation Loss: 1.32\n",
      "  Validation took: 0:00:49\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    242.    Elapsed: 0:01:36.\n",
      "  Batch    80  of    242.    Elapsed: 0:03:13.\n",
      "  Batch   120  of    242.    Elapsed: 0:04:49.\n",
      "  Batch   160  of    242.    Elapsed: 0:06:25.\n",
      "  Batch   200  of    242.    Elapsed: 0:08:01.\n",
      "  Batch   240  of    242.    Elapsed: 0:09:37.\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Training epcoh took: 0:09:41\n",
      "\n",
      "Running Validation...\n",
      "  RMSE: 1.1461\n",
      "  Validation Loss: 1.32\n",
      "  Validation took: 0:00:49\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    242.    Elapsed: 0:01:36.\n",
      "  Batch    80  of    242.    Elapsed: 0:03:13.\n",
      "  Batch   120  of    242.    Elapsed: 0:04:49.\n",
      "  Batch   160  of    242.    Elapsed: 0:06:25.\n",
      "  Batch   200  of    242.    Elapsed: 0:08:02.\n",
      "  Batch   240  of    242.    Elapsed: 0:09:38.\n",
      "\n",
      "  Average training loss: 1.32\n",
      "  Training epcoh took: 0:09:41\n",
      "\n",
      "Running Validation...\n",
      "  RMSE: 1.1461\n",
      "  Validation Loss: 1.32\n",
      "  Validation took: 0:00:49\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:42:00 (h:mm:ss)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPM_JyitC93v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
